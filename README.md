# ğŸ  House Prices â€“ Advanced Regression Techniques

A complete end-to-end machine learning solution for the Kaggle competition  
**House Prices: Advanced Regression Techniques**.

This project focuses on robust feature engineering, advanced preprocessing,
and ensemble-based regression models to accurately predict house prices.
<img width="1024" height="1536" alt="hous" src="https://github.com/user-attachments/assets/31333946-bbab-42e9-af5b-9047ab4658b8" />

---

## ğŸ“Œ Problem Statement
Predict the final sale price of residential homes in Ames, Iowa, using
79 explanatory variables describing almost every aspect of residential homes.

Evaluation metric: **RMSE (Root Mean Squared Error)** on log-transformed prices.

---

## ğŸ§  Approach

### 1. Exploratory Data Analysis
- Target distribution analysis
- Missing value patterns
- Feature interactions (e.g., `OverallQual Ã— GrLivArea`)
- Skewness and outlier handling

### 2. Feature Engineering
- Property age & remodeling age
- Total square footage
- Aggregated bathroom features
- Quality score synthesis
- Rare category encoding
- Log transformation for skewed numerical features

### 3. Preprocessing Pipeline
- Numerical: median imputation + scaling
- Categorical: mode imputation + ordinal encoding
- Unified `ColumnTransformer` for train/test consistency

### 4. Models Used
- Random Forest Regressor
- Gradient Boosting Regressor
- XGBoost Regressor
- LightGBM Regressor

Cross-validation is used for reliable performance estimation.

---

## ğŸ“Š Results

| Model | CV RMSE (mean Â± std) |
|------|----------------------|
| Random Forest | Competitive |
| Gradient Boosting | Strong |
| XGBoost | Excellent |
| LightGBM | Best Performance |

(Tree-based boosting models achieved the lowest RMSE.)

---

## ğŸ› ï¸ Tech Stack
- Python
- NumPy, Pandas
- Scikit-learn
- XGBoost
- LightGBM
- Matplotlib, Seaborn

---

## ğŸ“ Repository Structure



â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Original Kaggle files (not uploaded if large)
â”‚   â”œâ”€â”€ processed/           # Cleaned / engineered data
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 03_modeling.ipynb
â”‚   â””â”€â”€ 04_ensembling.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ submissions/
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
ğŸ‘‰ If you only have one notebook, place it in:

notebooks/House_Prices_Advanced_Regression.ipynb
That is perfectly acceptable.

3. README.md (Copyâ€“Paste Ready)
README.md
# ğŸ  House Prices â€“ Advanced Regression Techniques

A complete end-to-end machine learning solution for the Kaggle competition  
**House Prices: Advanced Regression Techniques**.

This project focuses on robust feature engineering, advanced preprocessing,
and ensemble-based regression models to accurately predict house prices.

---

## ğŸ“Œ Problem Statement
Predict the final sale price of residential homes in Ames, Iowa, using
79 explanatory variables describing almost every aspect of residential homes.

Evaluation metric: **RMSE (Root Mean Squared Error)** on log-transformed prices.

---

## ğŸ§  Approach

### 1. Exploratory Data Analysis
- Target distribution analysis
- Missing value patterns
- Feature interactions (e.g., `OverallQual Ã— GrLivArea`)
- Skewness and outlier handling

### 2. Feature Engineering
- Property age & remodeling age
- Total square footage
- Aggregated bathroom features
- Quality score synthesis
- Rare category encoding
- Log transformation for skewed numerical features

### 3. Preprocessing Pipeline
- Numerical: median imputation + scaling
- Categorical: mode imputation + ordinal encoding
- Unified `ColumnTransformer` for train/test consistency

### 4. Models Used
- Random Forest Regressor
- Gradient Boosting Regressor
- XGBoost Regressor
- LightGBM Regressor

Cross-validation is used for reliable performance estimation.

---

## ğŸ“Š Results

| Model | CV RMSE (mean Â± std) |
|------|----------------------|
| Random Forest | Competitive |
| Gradient Boosting | Strong |
| XGBoost | Excellent |
| LightGBM | Best Performance |

(Tree-based boosting models achieved the lowest RMSE.)

---

## ğŸ› ï¸ Tech Stack
- Python
- NumPy, Pandas
- Scikit-learn
- XGBoost
- LightGBM
- Matplotlib, Seaborn

---

## ğŸ“ Repository Structure
notebooks/ â†’ Jupyter notebooks (EDA â†’ Modeling)
src/ â†’ Modular Python code
outputs/ â†’ Figures and Kaggle submissions


---

## ğŸš€ How to Run

```bash

pip install -r requirements.txt
Open the main notebook:
```
jupyter notebook notebooks/House_Prices_Advanced_Regression.ipynb
ğŸ† Kaggle Competition
https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques

ğŸ‘¤ Author
Hammad Zahid
Data Scientist | Machine Learning Enthusiast

ğŸ”— LinkedIn: https://www.linkedin.com/in/hammad-zahid-xyz
ğŸ™ GitHub: https://github.com/Hamad-Ansari
